{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINE UTILS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tocken_index(samples):\n",
    "    token_index = {}\n",
    "    for sample in samples:\n",
    "        for word in sample.split():\n",
    "            if word not in token_index:\n",
    "                token_index[word] = len(token_index) + 1\n",
    "\n",
    "    max_length = 10\n",
    "    \n",
    "    return token_index\n",
    "\n",
    "def get_one_hot(samples, token_index, max_length, do_type=\"word\"):\n",
    "    results = np.zeros((len(samples), max_length, max(token_index.values())))\n",
    "    for i, sample in enumerate(samples):\n",
    "        if do_type == \"word\":\n",
    "            for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "                results[i, j, token_index.get(word)-1] = 1\n",
    "        elif do_type == \"charactor\":\n",
    "            for j, charactor in enumerate(sample[:max_length]):\n",
    "                results[i, j, token_index.get(charactor)] = 1\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO TEST1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "\n",
    "samples1 = ['The cat sat on the mat.',\n",
    "           'The dog ate my homework.']\n",
    "\n",
    "samples2 = ['The cat sat on the mat.',\n",
    "           'The dog ate my homework.',\n",
    "           'a panda is sleeping']\n",
    "\n",
    "# parameter\n",
    "\n",
    "max_length1 = 10\n",
    "max_length2 = 50\n",
    "\n",
    "characters = string.printable\n",
    "\n",
    "maxlen = 20\n",
    "max_features = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_index :  {'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework.': 10}\n",
      "\n",
      "results[1, 1] :  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# test1\n",
    "\n",
    "token_index = get_tocken_index(samples1)\n",
    "results = get_one_hot(samples1, token_index, max_length1)\n",
    "        \n",
    "print(\"token_index : \", token_index)\n",
    "print(\"\")\n",
    "print(\"results[1, 1] : \", results[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_index :  {'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework.': 10, 'a': 11, 'panda': 12, 'is': 13, 'sleeping': 14}\n",
      "\n",
      "results[1, 1] :  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# test2\n",
    "\n",
    "token_index = get_tocken_index(samples2)\n",
    "results = get_one_hot(samples2, token_index, max_length1)\n",
    "        \n",
    "print(\"token_index : \", token_index)\n",
    "print(\"\")\n",
    "print(\"results[1, 1] : \", results[1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO TEST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_index :  {'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36, 'A': 37, 'B': 38, 'C': 39, 'D': 40, 'E': 41, 'F': 42, 'G': 43, 'H': 44, 'I': 45, 'J': 46, 'K': 47, 'L': 48, 'M': 49, 'N': 50, 'O': 51, 'P': 52, 'Q': 53, 'R': 54, 'S': 55, 'T': 56, 'U': 57, 'V': 58, 'W': 59, 'X': 60, 'Y': 61, 'Z': 62, '!': 63, '\"': 64, '#': 65, '$': 66, '%': 67, '&': 68, \"'\": 69, '(': 70, ')': 71, '*': 72, '+': 73, ',': 74, '-': 75, '.': 76, '/': 77, ':': 78, ';': 79, '<': 80, '=': 81, '>': 82, '?': 83, '@': 84, '[': 85, '\\\\': 86, ']': 87, '^': 88, '_': 89, '`': 90, '{': 91, '|': 92, '}': 93, '~': 94, ' ': 95, '\\t': 96, '\\n': 97, '\\r': 98, '\\x0b': 99, '\\x0c': 100}\n",
      "\n",
      "results[1, 1] :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# test1\n",
    "\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "results = get_one_hot(samples1, token_index, max_length2, \"charactor\")\n",
    "\n",
    "print(\"token_index : \", token_index)\n",
    "print(\"\")\n",
    "print(\"results[1, 1] : \", results[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_index :  {'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36, 'A': 37, 'B': 38, 'C': 39, 'D': 40, 'E': 41, 'F': 42, 'G': 43, 'H': 44, 'I': 45, 'J': 46, 'K': 47, 'L': 48, 'M': 49, 'N': 50, 'O': 51, 'P': 52, 'Q': 53, 'R': 54, 'S': 55, 'T': 56, 'U': 57, 'V': 58, 'W': 59, 'X': 60, 'Y': 61, 'Z': 62, '!': 63, '\"': 64, '#': 65, '$': 66, '%': 67, '&': 68, \"'\": 69, '(': 70, ')': 71, '*': 72, '+': 73, ',': 74, '-': 75, '.': 76, '/': 77, ':': 78, ';': 79, '<': 80, '=': 81, '>': 82, '?': 83, '@': 84, '[': 85, '\\\\': 86, ']': 87, '^': 88, '_': 89, '`': 90, '{': 91, '|': 92, '}': 93, '~': 94, ' ': 95, '\\t': 96, '\\n': 97, '\\r': 98, '\\x0b': 99, '\\x0c': 100}\n",
      "\n",
      "results[1, 1] :  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# test2\n",
    "\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "results = get_one_hot(samples2, token_index, max_length2, \"charactor\")\n",
    "\n",
    "print(\"token_index : \", token_index)\n",
    "print(\"\")\n",
    "print(\"results[1, 1] : \", results[1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO TEST3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index :  {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n",
      "\n",
      "results[1, 1] :  [0. 1. 0. 0. 0. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples1)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples1)\n",
    "results = tokenizer.texts_to_matrix(samples1, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"word_index : \", word_index)\n",
    "print(\"\")\n",
    "print(\"results[1, 1] : \", results[1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape (25000, 20)\n",
      "y_train.shape (25000,)\n",
      "x_test.shape (25000, 20)\n",
      "y_test.shape (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train.shape\", x_train.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"x_test.shape\", x_test.shape)\n",
    "print(\"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 32)            320000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 641       \n",
      "=================================================================\n",
      "Total params: 320,641\n",
      "Trainable params: 320,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=maxlen),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "   32/20000 [..............................] - ETA: 38s - loss: 0.6948 - acc: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhq12\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.6268 - acc: 0.6660 - val_loss: 0.5392 - val_acc: 0.7236\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4673 - acc: 0.7811 - val_loss: 0.4933 - val_acc: 0.7504\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.4037 - acc: 0.8183 - val_loss: 0.4908 - val_acc: 0.7566\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.3564 - acc: 0.8463 - val_loss: 0.4982 - val_acc: 0.7586\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.3118 - acc: 0.8730 - val_loss: 0.5066 - val_acc: 0.7488\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.2688 - acc: 0.8968 - val_loss: 0.5218 - val_acc: 0.7468\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 3s 143us/step - loss: 0.2278 - acc: 0.9164 - val_loss: 0.5427 - val_acc: 0.7424\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.1908 - acc: 0.9344 - val_loss: 0.5654 - val_acc: 0.7336\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.1583 - acc: 0.9488 - val_loss: 0.5923 - val_acc: 0.7314\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.1304 - acc: 0.9614 - val_loss: 0.6171 - val_acc: 0.7288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16c4743af60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "         epochs=10,\n",
    "         batch_size=32,\n",
    "         validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
